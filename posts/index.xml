<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on 6equj5.dev</title>
    <link>https://blog.6equj5.dev/posts/</link>
    <description>Recent content in Articles on 6equj5.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Dec 2019 12:00:00 -0500</lastBuildDate>
    
	<atom:link href="https://blog.6equj5.dev/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Simple distributed tracing with OpenTracing and Stackdriver</title>
      <link>https://blog.6equj5.dev/posts/opentelemetry_stackdriver/</link>
      <pubDate>Tue, 24 Dec 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/opentelemetry_stackdriver/</guid>
      <description>Nothing much, just my variation/helloworld for opentelemetry in golang..its my variation of Opentelemetry-Distributed Tracing sample
This is a simple frontend-backend application you can run on your laptop which demonstrates distributed tracing between microservices.
What step 5 below shows is an inbound request to one microservice (/frontend) which emits some subspans, then makes an http call to a backend app (/backend) which also emits some spans. The final trace you see is a combined end-to-end trace between microservices.</description>
    </item>
    
    <item>
      <title>TPM2-TSS-Engine hello world and Google Cloud Authentication </title>
      <link>https://blog.6equj5.dev/posts/tpm2_evp_sign_decrypt/</link>
      <pubDate>Thu, 28 Nov 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/tpm2_evp_sign_decrypt/</guid>
      <description>Basic application that uses a the tpm2-tss-engine to perform RSA encryption and signatures.
This is intended to run on a system with a TPM as well as the the openssl engine library installed. The TPM-based private key is generated directly using tpm2tss-genkey and from that, the openssl engine to surface the public part. The tpm2-tss-engine surfaces the OpenSSL constructs like EVP_PKEY_RSA so you can directly use that against the TPM</description>
    </item>
    
    <item>
      <title>Google Cloud SSH with OS-Login with YubiKey OpenSC-PKCS11 and Trusted Platform Module (TPM) based keys</title>
      <link>https://blog.6equj5.dev/posts/gpg_tpm_ssh/</link>
      <pubDate>Mon, 04 Nov 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gpg_tpm_ssh/</guid>
      <description>or &amp;ldquo;How to embed SSH private keys into a Yubikey or TPM&amp;rdquo;. First off, this is nothing new; its a rehash of decade old tech that i decided to try out since i happens to have a YubiKey Neo and familiarity with Trusted Platform Module on a GCP Shielded VM.
Both the Yubikey Neo and TPM have one common capability here which this tutorial covers: embedding an RSA private key inextricably into a hardware device and provide an interface to sign arbitrary data using that key.</description>
    </item>
    
    <item>
      <title>Envoy Global rate limiting helloworld</title>
      <link>https://blog.6equj5.dev/posts/envoy_ratelimit/</link>
      <pubDate>Tue, 22 Oct 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/envoy_ratelimit/</guid>
      <description>Sample &amp;ldquo;hello world&amp;rdquo; application demonstrating basic aspects fo Envoy&amp;rsquo;s Global Rate Limiting capability. This app configures a local envoy instance for an HTTP level rate limit filter and then uses Lyft&amp;rsquo;s RateLimit Service as the backend.
Envoy&amp;rsquo;s rate limit functions checks each request to see if it should go through or not either using a local instance specific criteria or globally. By local, the rate limit counter runs within the context of the single envoy proxy that handles the request.</description>
    </item>
    
    <item>
      <title>Writing Developer logs with Google Cloud Logging</title>
      <link>https://blog.6equj5.dev/posts/writing_developer_logs_gcp/</link>
      <pubDate>Mon, 01 Jul 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/writing_developer_logs_gcp/</guid>
      <description>Several months ago Google Cloud Logging introduced two new monitored resource types geared towards allowing developers to emit cloud logging messages for their own application centric logs. Pereviously, application logs generally had to be tied to existing predefined monitored_resources such as GCE, GKE, AppEngine, Dataflow and so on. Under those monitoried resources sources, multiple log entries were attributed to specific logNames describing the subsystem like syslog, apache2, nginx, mysql, etc.</description>
    </item>
    
    <item>
      <title>Calling Cloud Composer &gt; GCF &gt; Composer securely</title>
      <link>https://blog.6equj5.dev/posts/composer_gcf/</link>
      <pubDate>Thu, 30 May 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/composer_gcf/</guid>
      <description>Sample Cloud Composer (Apache Airflow) configuration to securely invoke Cloud Functions or Cloud Run.
In addition this sample shows inverse: how Cloud Functions can invoke a Composer DAG securely. While GCF-&amp;gt;Composer is documented here, the configuration detailed here is minimal and (to me), easier to read.
Anyway, the following will setup cloud composer, then we will trigger composer to invoke a cloud function&amp;hellip;the cloud function will just trigger a different cloud composer endpoint&amp;hellip;.</description>
    </item>
    
    <item>
      <title>Google Cloud Trace context propagation and metrics graphs with Grafana&#43;Prometheus and Stackdriver</title>
      <link>https://blog.6equj5.dev/posts/cloud_trace/</link>
      <pubDate>Thu, 30 May 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/cloud_trace/</guid>
      <description>I wanted to understand how to setup a standalone golang app that integrated Opencensus specifically for Tracing and Metrics. The type of tracing i was after was both automatic and between web requests. By automatic i mean if you initialize opencensus and then directly use a supporting library to access a resrouce (eg. Google Cloud Storage client), tracing information about specific actions within the GCS call is rendered (eg, time taken for each individual get/put operation).</description>
    </item>
    
    <item>
      <title>Anti Virus file scanning on Google Cloud Storage using ClamAV</title>
      <link>https://blog.6equj5.dev/posts/clam_av/</link>
      <pubDate>Mon, 27 May 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/clam_av/</guid>
      <description>Tutorial on how to use ClamAV to scan files uploaded to Google Cloud Storage (GCS).
GCS does not have any built in capability to scan or do any other type of preprocessing on its files and relies on other services to perform these steps. In this tutorial, we will process a file that gets uploaded to GCS for viruses, malware, etc using ClamAV.
Architecture The basic flow outlined here is:</description>
    </item>
    
    <item>
      <title>Automatic OIDC:  Using Cloud Scheduler, Tasks, and PubSub to make authenticated calls to Cloud Run, Cloud Functions or your Server</title>
      <link>https://blog.6equj5.dev/posts/automatic_oidc/</link>
      <pubDate>Mon, 20 May 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/automatic_oidc/</guid>
      <description>This is a second in a series related to a versatile feature in Cloud Scheduler and Cloud Tasks and Cloud PubSub that automatically emits google OpenIDConnect and oauth2 access_token to outbound webhook calls. When a Scheduled task fires and calls an HTTP endpoint, it can automatically carry credentials to authenticate itself. The id_token credential can then get validated at the HTTP web-hook target using well known techniques (i.,e validate the signature and aud: fields in the token).</description>
    </item>
    
    <item>
      <title>Google Container Registry statistics from GCS access_logs</title>
      <link>https://blog.6equj5.dev/posts/gcr_stats/</link>
      <pubDate>Mon, 20 May 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gcr_stats/</guid>
      <description>Sample flow to extract Google Container Registry usage statistics (image push/pull counts, analytics, etc). GCR images are hosted on Google Cloud Storage which does have the ability to export usage which means we can indirectly acquire GCR&amp;rsquo;s usage.
There are several step sto getting the following pipeline to work but in is basic form, we setup GCS bucket used by GCR to export its usage stats to another GCS bucket.</description>
    </item>
    
    <item>
      <title>Automatic oauth2:  Using Cloud Scheduler and Tasks to call Google APIs</title>
      <link>https://blog.6equj5.dev/posts/automatic_oauth2/</link>
      <pubDate>Mon, 20 May 2019 13:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/automatic_oauth2/</guid>
      <description>A month ago or so I tried out a pretty versatile feature in Cloud Scheduler and Cloud Tasks and Cloud Tasks that emits OpenIDConnect or oauth2 access_token to outbound webhook calls.
When a Scheduled task fires and calls an HTTP endpoint, it can optionally automatically carry credentials for use with a GCP REST Endpoint. What does that mean? Well, you can automatically trigger most Google APIs directly do to any number of things on schedule or as a task instead of creating and running a cron elsewhere.</description>
    </item>
    
    <item>
      <title>GCP Binary Authorization and Vulnerability Scanning Demo</title>
      <link>https://blog.6equj5.dev/posts/binary_auth_demo/</link>
      <pubDate>Wed, 01 May 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/binary_auth_demo/</guid>
      <description>Sample application for GCP Binary Authorization + Vulernability Scanner.
 Creates GKE cluster with Binary Authorization Enables Binary Authoriztion policy on cluster Cloud Builder workflow checks checked in code to Cloud Source repository  Builds container image and pushes to Cloud Container Registry Binary Auhorization step waits for Vulernability Scan to complete If Vulernability Scanner Fails, Deployment Fails If Succeeds, the imae is authorized for deployment to GKE    Note: this sample uses one cloud builder configuration to both build the image and do attestation.</description>
    </item>
    
    <item>
      <title>GPG stream encryption and decryption on Google Cloud Functions and Cloud Run</title>
      <link>https://blog.6equj5.dev/posts/gpg_gcf/</link>
      <pubDate>Sun, 28 Apr 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gpg_gcf/</guid>
      <description>About a year+ ago a colleague of mine (Daz Wilkin) asked me how best to decompress/unzip a file using Google Cloud Functions. The suggestion ended as a sample he provided demonstrating the stream-read-&amp;gt;stream-write pattern with the pertinent input-outputs (input: unziped file; output zipped file). The distinct advantage of stream processing the unzip function is that the data is never held in memory: as the unzipped content gets processed by GCF, its promptly written as a zip file to GCS.</description>
    </item>
    
    <item>
      <title>Terraform &#39;Assume Role&#39; and service Account impersonation on Google Cloud</title>
      <link>https://blog.6equj5.dev/posts/terraform_gcp_impersonation/</link>
      <pubDate>Sun, 28 Apr 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/terraform_gcp_impersonation/</guid>
      <description>About two months ago, someone asked me to help them setup Terraform to automatically provision GCP projects. It was the first time I actually used it and found it capabilities pretty powerful: it&amp;rsquo;s easy to manage complex resources and maintain a picture of the state change. However, one aspect of its capabilities that struck me was its need to directly have permissions on all GCP resources it provisioned or manage.</description>
    </item>
    
    <item>
      <title>Upload/Download files from a browser with GCS Signed URLs and Signed Policy Documents</title>
      <link>https://blog.6equj5.dev/posts/gcs_post_signedurl_js/</link>
      <pubDate>Sun, 28 Apr 2019 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gcs_post_signedurl_js/</guid>
      <description>Small javascript application showing how to upload/download files with GCS Signed URLs and Signed Policy Documents. This article will not cover in detail what those two mechanisms are but rather demonstrate a basic application that exercises both on the browser. This is a simple client-server app that uploads files using these two mechanisms from a user&amp;rsquo;s browser. SignedURLs w/ javascript has been done many times before (see references); this article describes SignedURLs and Policy document differences and implementations.</description>
    </item>
    
    <item>
      <title>Proxy container for Google Cloud Platofrm pubsub and datastore emulators.</title>
      <link>https://blog.6equj5.dev/posts/gcpemulator/</link>
      <pubDate>Sun, 11 Sep 2016 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gcpemulator/</guid>
      <description>NOTE: 12/30/19 : This article is very old; while it probably does run to some extent, i&amp;rsquo;m not sure if the tooling is still needed. I&amp;rsquo;ve removed this from GCP Blog to reduce some noise; i&amp;rsquo;m keeping it in my archive here&amp;hellip;.
  Proxy server and container for GCP Cloud PubSub and Cloud Datastore emulators. In addition, the container runs an nginx HTTPS proxy infront of the PubSub|Datastore emulators so that even direct references to pubsub.</description>
    </item>
    
    <item>
      <title>Google Cloud Platform API hello world samples</title>
      <link>https://blog.6equj5.dev/posts/gcpsamples/</link>
      <pubDate>Mon, 25 Jul 2016 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gcpsamples/</guid>
      <description>This article describes the various mechanisms to access GCP Services using our APIs. I find it pretty confusing to keep track of all the various ways to access a service and that coupled with the changes in the library set accross languages, i often lose trac….so I’ve kept this repo as a running reference. Hope you find some of the samples useful.
This article will only describe the libraries in general terms but point back to a gitRepo for all the code samples.</description>
    </item>
    
    <item>
      <title>Kubernetes Services HelloWorld</title>
      <link>https://blog.6equj5.dev/posts/kubehelloworld/</link>
      <pubDate>Tue, 19 Jul 2016 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/kubehelloworld/</guid>
      <description>NOTE: 12/20/19: While this demo will work, its quite dated (i wrote is this &amp;lsquo;16..there are much more comprehensive articles that cover this..)
 Anyway&amp;hellip;
Sample application that deploys a trivial Kubernetes service to connect a frontend system (fe) with a backend (be).
This is merely to demonstrate Kubernetes service discovery in Google Container Engine (GKE), nothing more and is based off the guestbook service example.
Parts of this sample code is from the default kubernetes &amp;lsquo;environment-guide&amp;rsquo; example.</description>
    </item>
    
    <item>
      <title>.NET on GCP</title>
      <link>https://blog.6equj5.dev/posts/gcpdotnet/</link>
      <pubDate>Sun, 24 Apr 2016 14:34:01 -0800</pubDate>
      
      <guid>https://blog.6equj5.dev/posts/gcpdotnet/</guid>
      <description>12/30/19: NOTE: as you can clearly tell, this article is dated. DO NOT use this (i&amp;rsquo;m leavig it up as a matter of record)
 Sample code demonstrating running trivial .NET web applications on Google Cloud Platform services.
These simply builds off of existing technologies and samples but configures it to run on GCP effeciently with healh checking and load balancing.
The example here uses Microsofts&amp;rsquo;s .NET Core 1.</description>
    </item>
    
  </channel>
</rss>